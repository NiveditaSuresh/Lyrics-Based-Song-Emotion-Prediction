{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import string \n",
    "import nltk\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from string import punctuation\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "le = preprocessing.LabelEncoder()\n",
    "C_vec = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = data.iloc[:,2]\n",
    "lyric_list = []\n",
    "for URL in url:\n",
    "    page = requests.get(URL)\n",
    "    html = BeautifulSoup(page.text, \"html.parser\") # Extract the page's HTML as a string\n",
    "\n",
    "    # Scrape the song lyrics from the HTML\n",
    "    lyrics = html.find(\"div\", class_=\"lyrics\").get_text()\n",
    "    lyric_list.append(lyrics)\n",
    "%store lyric_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022\n"
     ]
    }
   ],
   "source": [
    "%store -r lyric_list\n",
    "print(len(lyric_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BASELINE APPROACH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROJECT CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('final_song_data.csv',delimiter=',',header =None)\n",
    "artist_name = data.iloc[:,0]\n",
    "song_name = data.iloc[:,1]\n",
    "data_set = [list(data_row) for data_row in zip(artist_name,song_name,lyric_list)]\n",
    "labels = le.fit_transform(data.iloc[:,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_features(data_set):\n",
    "    \n",
    "    lyric_list_new = []\n",
    "    artist_name_new = []\n",
    "    song_name_new = []\n",
    "    chorus_list = []\n",
    "    verse_list = []\n",
    "    bridge_list = []\n",
    "    hook_list = []\n",
    "    intro_list = []\n",
    "    outro_list = []\n",
    "    song_length = []\n",
    "    for dat in data_set:\n",
    "        s = dat[2]\n",
    "        chorus_list.append(len(re.findall('\\[Chorus', s)))\n",
    "        verse_list.append(len(re.findall('\\[Verse', s)))\n",
    "        hook_list.append(len(re.findall('\\[Hook', s)))\n",
    "        bridge_list.append(len(re.findall('\\[Bridge', s)))\n",
    "        intro_list.append(len(re.findall('\\[Intro', s)))\n",
    "        outro_list.append(len(re.findall('\\[Outro', s)))\n",
    "\n",
    "        v = s.split(\"\\n\")\n",
    "        song_length.append(len(v))\n",
    "        for i in v:\n",
    "            d = re.sub(\"^\\[(.*)]\",\"\",i)\n",
    "            v[v.index(i)] = d\n",
    "        lyric_list_new.append(\"\\n\".join(v))       \n",
    "        \n",
    "        artist_name_new.append(dat[0])\n",
    "        song_name_new.append(dat[1])\n",
    "\n",
    "   \n",
    "    for element in range(1,len(lyric_list_new)): \n",
    "        \n",
    "        #LYRICS\n",
    "        lyric_list_new[element] =lyric_list_new[element].replace(\"'\",\" \")\n",
    "        lyric_list_new[element] = re.sub(r'[^\\w\\s]','',lyric_list_new[element])\n",
    "        lyric_list_new[element] = nltk.word_tokenize(lyric_list_new[element])\n",
    "        not_stopword_list = [word.lower() for word in lyric_list_new[element] if word.lower() not in stopword]\n",
    "        t = \" \".join([wn.lemmatize(word) for word in not_stopword_list])\n",
    "        lyric_list_new[element]=t\n",
    "        \n",
    "        #ARTIST\n",
    "        artist_name_new[element] =artist_name_new[element].replace(\",\",\"\")\n",
    "        artist_name_new[element] =artist_name_new[element].replace(\"&\",\"\")\n",
    "\n",
    "        \n",
    "        #SONG TITLE\n",
    "        song_name_new[element] =song_name_new[element].replace(\"'\",\" \")\n",
    "        song_name_new[element] = re.sub(r'[^\\w\\s]','',song_name_new[element])\n",
    "        song_name_new[element] = nltk.word_tokenize(song_name_new[element])\n",
    "        not_stopword_list = [word.lower() for word in song_name_new[element] if word.lower() not in stopword]\n",
    "        t = \" \".join([wn.lemmatize(word) for word in not_stopword_list])\n",
    "        song_name_new[element]=t\n",
    "        \n",
    "    data_set_new = [artist_name_new,song_name_new,lyric_list_new]\n",
    "    lyric_all = np.vstack((np.array(chorus_list),np.array(verse_list),np.array(bridge_list),np.array(hook_list),np.array(intro_list),np.array(outro_list),np.array(song_length)))\n",
    "    lyric_all = np.transpose(lyric_all)\n",
    "    return data_set_new,lyric_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data_set,labels, train_size=0.7,test_size = 0.3)\n",
    "\n",
    "#Training Set \n",
    "dataset_train_sub,feat_add_train = process_features(x_train)\n",
    "artist_train_sub = dataset_train_sub[0]\n",
    "song_title_train_sub = dataset_train_sub[1]\n",
    "lyric_train_sub = dataset_train_sub[2]\n",
    "lyric_train_label = y_train\n",
    "\n",
    "\n",
    "#Test set \n",
    "dataset_test_sub,feat_add_test = process_features(x_test)\n",
    "artist_test_sub = dataset_test_sub[0]\n",
    "song_title_test_sub = dataset_test_sub[1]\n",
    "lyric_test_sub = dataset_test_sub[2]\n",
    "lyric_test_label = y_test\n",
    "\n",
    "#FEATURE EXTRACTION\n",
    "\n",
    "trainX1 = C_vec.fit_transform(lyric_train_sub).toarray()\n",
    "testX1 = C_vec.transform(lyric_test_sub).toarray()\n",
    "\n",
    "trainX2 = C_vec.fit_transform(artist_train_sub).toarray()\n",
    "testX2 = C_vec.transform(artist_test_sub).toarray()\n",
    "\n",
    "trainX3 = C_vec.fit_transform(song_title_train_sub).toarray()\n",
    "testX3 = C_vec.transform(song_title_test_sub).toarray()\n",
    "\n",
    "lyric_train = np.hstack((trainX1,trainX2,trainX3,feat_add_train))\n",
    "lyric_test = np.hstack((testX1,testX2,testX3,feat_add_test))\n",
    "\n",
    "\n",
    "#Final feature : lyric_train\n",
    "print(\"Train Data Shape: \",np.shape(lyric_train))\n",
    "print(\"Test Data Shape: \",np.shape(lyric_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSIFIERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MULTINOMIAL NAIVE BAYES\n",
    "\n",
    "nb_g = GaussianNB()\n",
    "model =nb_g.fit(lyric_train,lyric_train_label)\n",
    "lyric_train_label_predict = model.predict(lyric_train)\n",
    "accuracy_train = accuracy_score(lyric_train_label,lyric_train_label_predict)\n",
    "lyric_test_label_predict = model.predict(lyric_test)\n",
    "accuracy_test = accuracy_score(lyric_test_label,lyric_test_label_predict)\n",
    "# print(lyric_test_label,lyric_test_label_predict)\n",
    "print('GaussianNB_Accuracy_Train = ',accuracy_train)\n",
    "print('GaussianNB_Accuracy_Test = ',accuracy_test)\n",
    "print('F1_Score = ',f1_score(lyric_test_label,lyric_test_label_predict,average = 'macro') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SUPPORT VECTOR MACHINES\n",
    "\n",
    "clf = SVC()\n",
    "model = clf.fit(lyric_train,lyric_train_label)\n",
    "lyric_train_label_predict = model.predict(lyric_train)\n",
    "accuracy_train = accuracy_score(lyric_train_label,lyric_train_label_predict)\n",
    "lyric_test_label_predict = model.predict(lyric_test)\n",
    "accuracy_test = accuracy_score(lyric_test_label,lyric_test_label_predict)\n",
    "# print(lyric_test_label,lyric_test_label_predict)\n",
    "print('SVM_Accuracy_Train = ',accuracy_train)\n",
    "print('SVM_Accuracy_Test = ',accuracy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOGISTICAL REGRESSION\n",
    "\n",
    "#Cross Validation for LogisticRegression\n",
    "\n",
    "feat_train,x_validation,feat_label,label_validation = train_test_split(lyric_train,lyric_train_label,test_size = 0.25,shuffle = True)\n",
    "max_acc = 0\n",
    "for c in np.linspace(0.001,1000,10):\n",
    "    for p in ['l1','l2']:\n",
    "\n",
    "        Lregression = LogisticRegression(multi_class = 'ovr',C=c,penalty = p)\n",
    "        OneVRest = OneVsRestClassifier(Lregression, n_jobs=-1)\n",
    "        model = OneVRest.fit(feat_train,feat_label)\n",
    "        label_validation_predict = model.predict(x_validation)\n",
    "        accuracy = accuracy_score(label_validation,label_validation_predict)\n",
    "        print('C := ',c, '  Penalty:= ',p,'  Accuracy= ',accuracy)\n",
    "        if(accuracy>=max_acc):\n",
    "            max_acc = accuracy\n",
    "            optimal_C = c\n",
    "            optimal_P = p\n",
    "\n",
    "Lregression = LogisticRegression(multi_class = 'ovr',C = optimal_C,penalty = optimal_P)\n",
    "OneVRest = OneVsRestClassifier(Lregression, n_jobs=-1)\n",
    "model = OneVRest.fit(feat_train,feat_label)\n",
    "label_validation_predict = model.predict(x_validation)\n",
    "accuracy = accuracy_score(label_validation,label_validation_predict)\n",
    "print('Accuracy= ',accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RANDOM FOREST\n",
    "\n",
    "feat_train,x_validation,feat_label,label_validation = train_test_split(lyric_train,lyric_train_label,test_size = 0.25,shuffle = True)\n",
    "max_acc = 0\n",
    "for n in range(1,31):\n",
    "        rfc = RandomForestClassifier(n_estimators = n)\n",
    "        model = rfc.fit(feat_train, feat_label)\n",
    "        label_validation_predict = model.predict(x_validation)\n",
    "        accuracy = accuracy_score(label_validation,label_validation_predict)\n",
    "        if(accuracy>=max_acc):\n",
    "            max_acc = accuracy\n",
    "            optimal_n = n\n",
    "rfc = RandomForestClassifier(n_estimators = optimal_n)\n",
    "model = rfc.fit(lyric_train, lyric_train_label)\n",
    "lyric_train_label_predict = model.predict(lyric_train)\n",
    "accuracy_train = accuracy_score(lyric_train_label,lyric_train_label_predict)\n",
    "lyric_test_label_predict = model.predict(lyric_test)\n",
    "accuracy_test = accuracy_score(lyric_test_label,lyric_test_label_predict)\n",
    "print('RF_Accuracy_Train = ',accuracy_train)\n",
    "print('RF_Accuracy_Test = ',accuracy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CONVOLUTIONAL NEURAL NETWORKS\n",
    "import tensorflow\n",
    "import keras\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt \n",
    "import keras\n",
    "import sys\n",
    "import math\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "np.random.seed(123)\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(123)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout, Activation,Flatten\n",
    "from keras.layers import Conv1D,MaxPooling1D\n",
    "from keras import optimizers\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "\n",
    "##(feat_train,l_train),(feat_test,l_test) = data\n",
    "##feat_train = feat_train.reshape(feat_train.shape[0],28,28,1)\n",
    "##feat_test = feat_test.reshape(feat_test.shape[0],28,28,1)\n",
    "feat_train = lyric_train.astype('float32')\n",
    "feat_test = lyric_test.astype('float32')\n",
    "##feat_train/=255\n",
    "##feat_test/=255\n",
    "label_train = to_categorical(lyric_train_label,3)\n",
    "label_test = to_categorical(lyric_test_label,3)\n",
    "feat_train_shape = feat_train.shape\n",
    "feat_test_shape = feat_test.shape\n",
    "print(feat_train_shape,len(feat_train))\n",
    "print(feat_test_shape)\n",
    "print(label_train.shape)\n",
    "print(label_test.shape)\n",
    "\n",
    "nb_features = int(feat_train_shape[1]/3)\n",
    "# number of features per features type (shape, texture, margin)   \n",
    "classes = [0,1,2]\n",
    "nb_class = 3\n",
    "\n",
    "# reshape train data\n",
    "# feat_train_r = np.zeros((len(feat_train), nb_features, 3))\n",
    "# feat_train_r[:, :, 0] = feat_train[:, :nb_features]\n",
    "# feat_train_r[:, :, 1] = feat_train[:, nb_features:(2*nb_features)+1]\n",
    "# feat_train_r[:, :, 2] = feat_train[:, (2*nb_features)+1:]\n",
    "\n",
    "# # reshape test data\n",
    "# feat_test_r = np.zeros((len(feat_test), nb_features, 3))\n",
    "# feat_test_r[:, :, 0] = feat_test[:, :nb_features]\n",
    "# feat_test_r[:, :, 1] = feat_test[:, nb_features:2830]\n",
    "# feat_test_r[:, :, 2] = feat_test[:, 2830:]\n",
    "\n",
    "feat_train = feat_train.reshape(feat_train.shape[0],142,149).astype('float32')\n",
    "feat_test = feat_test.reshape(feat_test.shape[0],142,149).astype('float32')\n",
    "\n",
    "# cnn_model = Sequential()\n",
    "\n",
    "# cnn_model.add(Conv1D(6, 5,padding='same', activation='relu', use_bias=True,input_shape = (feat_train_shape[0]\n",
    "#                                                                                           ,feat_train_shape[1]), kernel_initializer = 'RandomUniform'))\n",
    "# # print(cnn_model.output_shape)\n",
    "\n",
    "# # cnn_model.add(MaxPooling1D(pool_size=2))\n",
    "# # print(cnn_model.output_shape)\n",
    "\n",
    "# cnn_model.add(Conv1D(16, 5,padding='valid', activation='relu', use_bias=True))\n",
    "# # print(cnn_model.output_shape)\n",
    "\n",
    "# # cnn_model.add(MaxPooling1D(pool_size=2))\n",
    "# # print(cnn_model.output_shape)\n",
    "\n",
    "# cnn_model.add(Flatten())\n",
    "# cnn_model.add(Dense(120, activation='relu', use_bias=True))\n",
    "# cnn_model.add(Dropout(0.2))\n",
    "# # print(cnn_model.output_shape)\n",
    "# cnn_model.add(Dense(80, activation='relu', use_bias=True))\n",
    "# # print(cnn_model.output_shape)\n",
    "# cnn_model.add(Dense(10, activation='softmax'))\n",
    "# # print(cnn_model.output_shape)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=10, input_shape=(142,149),activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=5 ))\n",
    "model.add(Conv1D(filters =64, kernel_size = 8,padding='valid', activation='relu', use_bias=True))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1000, activation='relu',use_bias=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(80, activation='relu', use_bias=True))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "\n",
    "# #lr *= (1. / (1. + self.decay * self.iterations))\n",
    "learning_rate = 0.05\n",
    "decay_rate = learning_rate / 100\n",
    "momentum = 0.8\n",
    "# sgd = SGD(lr=0.01, nesterov=True, decay=1e-6, momentum=0.9)\n",
    "# model.compile(loss='categorical_crossentropy',optimizer=sgd,metrics=['accuracy'])\n",
    "\n",
    "sgd = optimizers.SGD(lr = learning_rate,momentum = momentum,decay=decay_rate)\n",
    "model.compile(loss='categorical_crossentropy',optimizer= sgd,metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(feat_train,label_train,epochs = 100, verbose = 2,validation_data = (feat_test,label_test))\n",
    "performance = model.evaluate(feat_test,label_test,verbose = 0)\n",
    "print('Test Loss:', performance[0])\n",
    "print('Test Accuracy: ', performance[1])\n",
    "performance = model.evaluate(feat_train,label_train,verbose = 0)\n",
    "print('Train Loss:', performance[0])\n",
    "print('Train Accuracy: ', performance[1])\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
